# Data analysis
- Document here the project: servier
- Description: DeepLearning for Molecular property prediction 
- Data Source: Provided by servier


# Stratup the project

The initial setup.

Create conda env and install:
```bash
$ wget --quiet https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
$ Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda
$ export PATH=~/miniconda/bin:$PATH
$ conda update -n base conda
$ conda create -y --name servier python=3.6
$ activate servier
$ conda install -c conda-forge rdkit
```
# Project Structure
```bash
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ MANIFEST.in
â”œâ”€â”€ Makefile                               ==> cookbook for reproducibility ;)
â”œâ”€â”€ README.md
â”œâ”€â”€ deploy                                  
â”‚Â Â  â”œâ”€â”€ api.py                             ==> flask API 
â”‚Â Â  â””â”€â”€ streamlit_app.py                   ==> Streamlit APP
â”œâ”€â”€ docker
â”‚Â Â  â”œâ”€â”€ Dockerfile.api                     ==> Docker for API deployment
â”‚Â Â  â””â”€â”€ Dockerfile.streamlit               ==> Docker for Streamlit deployment
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚Â Â  â””â”€â”€ servier
â”œâ”€â”€ servier                                 ==> python servier package
â”‚Â Â  â”œâ”€â”€ __init__.py                         
â”‚Â Â  â”œâ”€â”€ data/...                            ==> data and final models
â”‚Â Â  â”œâ”€â”€ data.py
â”‚Â Â  â”œâ”€â”€ dl.py
â”‚Â Â  â”œâ”€â”€ encoders.py                         ==> custom sklearn Pipeline encoders
â”‚Â Â  â”œâ”€â”€ feature_extractor.py                
â”‚Â Â  â”œâ”€â”€ jupy/...                            ==> exploratory notebooks
â”‚Â Â  â”œâ”€â”€ main.py                             ==> main trainer|evaluator|predictor class
â”‚Â Â  â”œâ”€â”€ plot.py
â”‚Â Â  â””â”€â”€ utils.py
â”œâ”€â”€ setup.py                                ==> keystone for packaging
â”œâ”€â”€ setup.sh                                ==> only here for streamlit docker deployment on Heroku
â””â”€â”€ tests                                   ==> test directory (to implement with more time)
    â”œâ”€â”€ __init__.py
    â””â”€â”€ lib_test.py
```

# Install
Go to `github.com/lologibus2/servier` to see the project

Activate conda env:
```bash
  $ conda activate servier 
```

Clone the project and install requirements:
```bash
  git clone git@github.com:lologibus2/servier.git
  cd servier
  pip install -r requirements.txt
```
Install package:
```bash
  make install clean
```

Scripts:
```bash
  cd /tmp
  servier train --model 1 -a mlp --split -p {PATH_TO_REPO}/servier/servier/data
  ls /tmp/models
```
```bash
  servier evaluate --model 1 -p {PATH_TO_REPO}/servier/servier/data
```

# API and streamlit app
Each API and streamlit app has been deployed on Heroku:  
 - Find the API ðŸ‘‰ [here](https://servier-api.herokuapp.com/)  (please click to wake heroku up)
 - And the streanlit app ðŸ‘‰ [There](https://servier-streamlit.herokuapp.com/)
 
Test api with this code snippet:
```python 
from servier.data import get_data
import requests

data_path = 'PATH_TO_REPO/servier/servier/data/'
api_url = "https://servier-api.herokuapp.com/"

df = get_data(path=data_path)
instances = df.drop("P1", axis=1).to_dict(orient="records")


print(r.json())
```
 
## Locally
API:
```bash
  make api_local 
```
Streamlit app:
```bash
  make streamlit_local
```
## Docker
API:
```bash
  make docker_build_api
  make docker_run_api
```
Streamlit app:
```bash
  make docker_build_streamlit
  make docker_run_streamlit
```

# Model Architecture
ðŸ‘‰ Both Model 1 and 2 are stored as sklearn pipeline objects and not directly models.  
 - The reason for that is to integrate both Preprocessing and model into final livrable    
 - Therefor both models take as an input original dataset, i.e smiles molecule representation
 
ðŸ‘‰ Model 1
 - Multi Layer Perceptron
 - Other standard ML model has been tried to compare (RandomForest for instance)
 
ðŸ‘‰ Model 2
 - 1D-CNN
 - Tried RNN with GRU and LSTM cells also, by lack of time did not have time to go Futher
 - To go further: combining MLP, RNN and CNN by concatenating final layers (initiated model but did not have timt to go )
 further)

ðŸ‘‰ Quick wins for better performances
 - Resample 
 - Data augmentation generating other smile sequence from original one

# Continus integration
## Github 
Every push of `master` branch will execute `.github/workflows/pythonpackages.yml` docker jobs.
